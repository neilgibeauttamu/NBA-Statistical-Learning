{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cell\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pprint as pp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "date_format = \"%Y-%m-%d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def executeCLF(clf, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train.values, y_train.values)\n",
    "    test_preds = clf.predict(x_test.values)\n",
    "    train_preds = clf.predict(x_train.values)\n",
    "    print(\"Accuracy on train set: \" + str(accuracy_score(y_train, train_preds))) \n",
    "    print(\"Accuracy on test set: \" + str(accuracy_score(y_test, test_preds))) \n",
    "    return pd.DataFrame({'PREDICTION': test_preds}), accuracy_score(y_test, test_preds)\n",
    "\n",
    "def printDetailedPreds(preds_df, y_test):\n",
    "    detailed_preds = pd.concat([preds_df.reset_index(drop=True), y_test.reset_index(drop=True)])\n",
    "    return detailed_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/outputs/final_norm.csv')\n",
    "df1 = pd.read_csv('../data/outputs/ranks.csv')\n",
    "game_id = df1[['GAME_ID_HOME']]\n",
    "\n",
    "df1_norm = df1.drop(['GAME_ID_HOME'],axis=1)\n",
    "df1_norm = (df1_norm - df1_norm.mean()) / (df1_norm.max() - df1_norm.min())\n",
    "df1 = pd.concat([game_id, df1_norm], axis=1)\n",
    "df = pd.merge(df, df1, left_on=['CURRENT_GAME_ID'], right_on=['GAME_ID_HOME'])\n",
    "\n",
    "train_start_date = datetime.strptime(\"2016-10-26\", date_format)\n",
    "train_end_date = datetime.strptime(\"2017-10-17\", date_format)\n",
    "test_date = datetime.strptime(\"2017-10-18\", date_format)\n",
    "\n",
    "date_list = df[\"GAME_DATE_EST\"][0:3269].drop_duplicates()\n",
    "\n",
    "test_end_index = int(df.index[df['GAME_DATE_EST'] == \"2018-04-10\"][-1])\n",
    "test_start_index = int(df.index[df['GAME_DATE_EST'] == \"2017-10-18\"][0])\n",
    "train_end_index = int(df.index[df['GAME_DATE_EST'] == \"2016-11-29\"][-1])\n",
    "train_start_index = int(df.index[df['GAME_DATE_EST'] == \"2017-04-08\"][0])\n",
    "\n",
    "temp = pd.get_dummies(df[['TEAM_NAME_AWAY']])\n",
    "df = df.drop(columns=['TEAM_NAME_AWAY'])\n",
    "df = pd.concat([df, temp], axis=1)\n",
    "\n",
    "temp = pd.get_dummies(df[['TEAM_NAME_HOME']])\n",
    "df = df.drop(columns=['TEAM_NAME_HOME'])\n",
    "df = pd.concat([df, temp], axis=1)\n",
    "df = df.drop(['GAME_DATE_EST','CURRENT_GAME_ID', 'GAME_ID_HOME'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Logistic Regression model:\n",
      "FOLD 1: \n",
      "Accuracy on train set: 0.6953045253487581\n",
      "Accuracy on test set: 0.7125382262996942\n",
      "\n",
      "\n",
      "FOLD 2: \n",
      "Accuracy on train set: 0.7094249744811161\n",
      "Accuracy on test set: 0.6422018348623854\n",
      "\n",
      "\n",
      "FOLD 3: \n",
      "Accuracy on train set: 0.7068730860837019\n",
      "Accuracy on test set: 0.6773700305810397\n",
      "\n",
      "\n",
      "FOLD 4: \n",
      "Accuracy on train set: 0.7007485539299081\n",
      "Accuracy on test set: 0.6865443425076453\n",
      "\n",
      "\n",
      "FOLD 5: \n",
      "Accuracy on train set: 0.6980265396393331\n",
      "Accuracy on test set: 0.6529051987767585\n",
      "\n",
      "\n",
      "FOLD 6: \n",
      "Accuracy on train set: 0.7017693092888738\n",
      "Accuracy on test set: 0.7048929663608563\n",
      "\n",
      "\n",
      "FOLD 7: \n",
      "Accuracy on train set: 0.6981292517006803\n",
      "Accuracy on test set: 0.7223926380368099\n",
      "\n",
      "\n",
      "FOLD 8: \n",
      "Accuracy on train set: 0.6979591836734694\n",
      "Accuracy on test set: 0.7208588957055214\n",
      "\n",
      "\n",
      "FOLD 9: \n",
      "Accuracy on train set: 0.6943877551020409\n",
      "Accuracy on test set: 0.7162576687116564\n",
      "\n",
      "\n",
      "FOLD 10: \n",
      "Accuracy on train set: 0.7042517006802721\n",
      "Accuracy on test set: 0.696319018404908\n",
      "\n",
      "\n",
      "AVG across folds: 0.6932280820247275\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "y = df['WIN_HOME']\n",
    "x = df.drop(columns=['WIN_HOME'])\n",
    "print(\"\\nResults for Logistic Regression model:\")\n",
    "skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "accuracies = []\n",
    "count = 1\n",
    "for train_index, test_index in skf.split(x, y):  \n",
    "    print('FOLD ' + str(count) + \": \")\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "  \n",
    "    log_clf = LogisticRegression(solver='lbfgs')\n",
    "    log_preds_df, accuracy = executeCLF(log_clf, x_train, y_train, x_test, y_test)\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"\\n\")\n",
    "    count+=1\n",
    "print('AVG across folds: ' + str(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Logistic Regression model:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-eb62d0651012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlogistic_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlog_preds_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecuteCLF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a5ed419ad9bd>\u001b[0m in \u001b[0;36mexecuteCLF\u001b[0;34m(clf, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecuteCLF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on train set: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    255\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 582\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# Train test split as 2017-2018 season\n",
    "y = df['WIN_HOME']\n",
    "x = df.drop(columns=['WIN_HOME'])\n",
    "print(\"\\nResults for Logistic Regression model:\")\n",
    "\n",
    "x_train, x_test = x.iloc[train_start_index:train_end_index], x.iloc[test_start_index:test_end_index]\n",
    "y_train, y_test = y.iloc[train_start_index:train_end_index], y.iloc[test_start_index:test_end_index]\n",
    "  \n",
    "logistic_reg = LogisticRegression(solver='lbfgs')\n",
    "log_preds_df, accuracy = executeCLF(logistic_reg, x_train, y_train, x_test, y_test)\n",
    "accuracies.append(accuracy)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Feature Selection \n",
    "\n",
    "train_x = x_train\n",
    "train_y = y_train\n",
    "test_x = x_test\n",
    "test_y = y_test\n",
    "\n",
    "# Initial calculation of p values\n",
    "pvalues = stats.ttest_ind(train_x, train_y)[1]\n",
    "\n",
    "best_logistic_r2 = 0\n",
    "best_logistic_model = logistic_reg\n",
    "best_logistic_x_train = train_x\n",
    "best_logistic_x_test = test_x\n",
    "\n",
    "while len(pvalues) > 1 :\n",
    "    # Drop highest p value\n",
    "    m = max(pvalues)\n",
    "    m_index = [i for i, j in enumerate(pvalues) if j == m]\n",
    "    train_x = train_x.drop(train_x.columns[m_index[0]], axis = 1)\n",
    "    test_x = test_x.drop(test_x.columns[m_index[0]], axis = 1)\n",
    "\n",
    "    # Recalculate p values after drop\n",
    "    pvalues = stats.ttest_ind(train_x, train_y)[1]\n",
    "    \n",
    "    # Fit model\n",
    "    new_logistic_reg = LogisticRegression(solver='lbfgs').fit(train_x.values, train_y.values)\n",
    "        \n",
    "    # Replace logistic model if new model is better \n",
    "    new_test_preds = new_logistic_reg.predict(test_x.values)  \n",
    "    new_logistic_r2 = accuracy_score(test_y, new_test_preds)\n",
    "    if new_logistic_r2 > best_logistic_r2:\n",
    "        best_logistic_r2 = new_logistic_r2\n",
    "        best_logistic_model = new_logistic_reg\n",
    "        best_logistic_x_train = train_x\n",
    "        best_logistic_x_test = test_x\n",
    "# Results of best models\n",
    "new_test_preds = best_logistic_model.predict(best_logistic_x_test.values)\n",
    "print(\"\\nResults after feature selection for logistic model:\")\n",
    "print(\"Accuracy on test set: \" + str(accuracy_score(test_y, new_test_preds))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "y = df['WIN_HOME']\n",
    "x = df.drop(columns=['WIN_HOME'])\n",
    "print(\"\\nResults for Random Forest model:\")\n",
    "skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "accuracies = []\n",
    "count = 1\n",
    "for train_index, test_index in skf.split(x, y):  \n",
    "    print('FOLD ' + str(count) + \": \")\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "  \n",
    "    rf_clf = RandomForestClassifier(n_estimators=20, max_depth=10)\n",
    "    rf_preds_df, accuracy = executeCLF(rf_clf, x_train, y_train, x_test, y_test)\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"\\n\")\n",
    "    count+=1\n",
    "print('AVG across folds: ' + str(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Random Forest model:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-45114a5aff87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrf_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mrf_preds_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecuteCLF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a5ed419ad9bd>\u001b[0m in \u001b[0;36mexecuteCLF\u001b[0;34m(clf, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecuteCLF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 582\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# Train test split as 2017-2018 season\n",
    "y = df['WIN_HOME']\n",
    "x = df.drop(columns=['WIN_HOME'])\n",
    "print(\"\\nResults for Random Forest model:\")\n",
    "\n",
    "x_train, x_test = x.iloc[train_start_index:train_end_index], x.iloc[test_start_index:test_end_index]\n",
    "y_train, y_test = y.iloc[train_start_index:train_end_index], y.iloc[test_start_index:test_end_index]\n",
    "for i in range(0, 10):\n",
    "    rf_reg = RandomForestClassifier(n_estimators=100, max_depth=11)\n",
    "    rf_preds_df, accuracy = executeCLF(rf_reg, x_train, y_train, x_test, y_test)\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ADABoost \n",
    "y = df['WIN_HOME']\n",
    "x = df.drop(columns=['WIN_HOME'])\n",
    "print(\"\\nResults for ADABoost model:\")\n",
    "skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "accuracies = []\n",
    "count = 1\n",
    "for train_index, test_index in skf.split(x, y):  \n",
    "    print('FOLD ' + str(count) + \": \")\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "  \n",
    "    rf_clf = AdaBoostClassifier(n_estimators=20)\n",
    "    rf_preds_df, accuracy = executeCLF(rf_clf, x_train, y_train, x_test, y_test)\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"\\n\")\n",
    "    count+=1\n",
    "print('AVG across folds: ' + str(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "y = df['WIN_HOME']\n",
    "x = df.drop(columns=['WIN_HOME'])\n",
    "print(\"\\nResults for AdaBoost model:\")\n",
    "\n",
    "x_train, x_test = x.iloc[train_start_index:train_end_index], x.iloc[test_start_index:test_end_index]\n",
    "y_train, y_test = y.iloc[train_start_index:train_end_index], y.iloc[test_start_index:test_end_index]\n",
    "  \n",
    "rf_reg = AdaBoostClassifier(n_estimators=20)\n",
    "rf_preds_df, accuracy = executeCLF(rf_reg, x_train, y_train, x_test, y_test)\n",
    "accuracies.append(accuracy)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
